Large Language Models are Few-shot
Testers: Exploring LLM-based General Bug
Reproduction (ICSE 2023)
Sungmin Kang, Juyeon Yoon, Shin Yoo (KAIST)

Link : https://scholar.google.co.kr/scholar?hl=ko&as_sdt=0%2C5&q=Large+Language+Models+are+Few-shot+Testers%3A+Exploring+LLM-based+General+Bug+Reproduction+%28ICSE+2023%29+Sungmin+Kang%2C+Juyeon+Yoon%2C+Shin+Yoo+%28KAIST%29&btnG=



요약 : LLM의 성능은 문제 해결을 요청하는 방법에 따라 성능이 크게 변하는데 성능을 높으기 위한 최적의 질문을 찾는 것을 프롬프트 엔지니어링이라고 한다. LIBRO는 LLM에 프롬프트 엔지니어링을 통해 만든 질문을 통해 버그 리포트로 부터 버그를 재생산하는 기술이다.

---

발표자 설명



버그재현테스트



LIBRO 4단계

프롬프트 생성

LLM Querying

테스트 후처리

랭킹 - 오류메세지가 비슷한 애들을 그룹화함

클러스터 사이즈 3인 애들 선택

테스트  오류 메세지가 버그 리포트와 매치되는 지 확인

세밀한 랭킹을 위해서 아웃풋 클러스터의 사이즈를 고려해서 사이즈가 큰걸 전달



Codex (OpenAI code-davinci-002) 이용



평가 벤치마크

Defects4j 2.0

GHRB

둘 모두에서 30% 넘게 버그 재생산



91개의 유니크 버그 (72개가 논크레시 버그)



Threshold = 1일때 고칠 수 잇는 버그의 수는 조금 줄었지만 정확성이 크게 늘었다,

----

## 요약

**개발자가 테스트를 작성하는 데 도움이 되도록 많은 자동화된 테스트 생성 기술이 개발되었습니다. 완전 자동화를 촉진하기 위해 대부분의 기존 기술은 적용 범위를 늘리거나 탐색 입력을 생성하는 것을 목표로 합니다. 그러나 기존의 테스트 생성 기술은 주어진 버그 보고서를 재현하기 위해 테스트를 생성하는 것과 같은 보다 의미론적인 목표를 달성하는 데 크게 부족합니다. 그럼에도 불구하고 버그를 재현하는 것은 중요합니다. 경험적 연구에 따르면 문제로 인해 오픈 소스 리포지토리에 추가된 테스트 수가 해당 프로젝트 테스트 스위트 크기의 약 28%인 것으로 나타났습니다. 한편, 버그 보고서에서 예상되는 프로그램 시맨틱을 테스트 오라클로 변환하는 데 어려움이 있기 때문에 기존 오류 재현 기술은 모든 버그 보고서의 작은 하위 집합인 프로그램 충돌을 독점적으로 처리하는 경향이 있습니다. 일반 버그 보고서에서 테스트 생성을 자동화하기 위해 코드 관련 작업을 수행할 수 있는 LLM(대형 언어 모델)을 사용하는 프레임워크인 LIBRO를 제안합니다. LLM 자체는 버그가 있는 대상 코드를 실행할 수 없기 때문에 LLM이 효과적인 시기를 식별하고 순위를 매기는 데 도움이 되는 후처리 단계에 중점을 둡니다.**
**유효성에 따라 생성된 테스트. LIBRO에 대한 우리의 평가는 광범위하게 연구된 Defects4J 벤치마크에서 LIBRO가 모든 연구 사례의 33%(750개 중 251개)에 대해 실패 재현 테스트 사례를 생성할 수 있는 반면, 149개의 버그에 대해 처음으로 버그 재현 테스트를 제안할 수 있음을 보여줍니다. 데이터 오염(즉, LLM이 단순히 테스트 코드를 부분적으로 또는 전체적으로 기억할 가능성)을 완화하기 위해 LLM 교육 데이터 수집이 종료된 후 제출된 31개의 버그 보고서에 대해 LIBRO를 평가합니다. LIBRO는 32개에 대한 버그 재현 테스트를 생성합니다. 조사된 버그 보고서의 %. 전반적으로 우리의 결과는 LIBRO가 버그 보고서에서 테스트를 자동으로 생성함으로써 개발자 효율성을 크게 향상시킬 수 있는 잠재력이 있음을 보여줍니다.**

---

## 서론

소프트웨어 테스팅은 테스트 중인 소프트웨어(SUT)에 대한 테스트를 실행하여 소프트웨어가 사양 기준을 충족하는지 확인하는 작업입니다. **많은 소프트웨어 프로젝트의 중요성과 안전이 중요한 특성으로 인해 소프트웨어 테스팅은 소프트웨어 개발 프로세스에서 가장 중요한 관행 중 하나입니다.** 그럼에도 불구하고 소프트웨어 테스팅은 상당한 인적 노력이 필요하기 때문에 지루하다는 것이 널리 알려져 있습니다[1]. 이 격차를 메우기 위해 자동화된 테스트 생성 기술이 거의 반세기 동안 연구되어 [2] 자동화 프로세스를 안내하기 위해 암시적 오라클(회귀 또는 충돌 감지)을 사용하는 많은 도구[3], [4]가 나왔습니다. 초점 클래스에 대한 높은 적용 범위로 새로운 테스트를 생성할 수 있으므로 새 기능이 추가될 때 유용합니다.
그러나 모든 테스트가 초점 클래스와 함께 즉시 추가되는 것은 아닙니다. 사실, **보고된 버그에 대한 향후 회귀를 방지하기 위해 생성된 상당한 수의 테스트가 버그 보고서에서 비롯된 것으로 나타났습니다.** 이는 버그 보고서에서 버그 재현 테스트를 생성하는 것이 과소 평가되었지만 개발자를 위한 테스트를 자동으로 작성하는 영향력 있는 방법임을 시사합니다. 우리의 주장은 JUnit을 사용한 300개의 오픈 소스 프로젝트 샘플 분석을 기반으로 합니다. 버그 보고의 결과로 추가된 테스트 수는 전체 테스트 스위트 크기의 중앙값 28%였습니다. 따라서 버그 보고-테스트 문제는 개발자가 정기적으로 다루며 자동화된 기술이 상당한 도움을 줄 수 있는 문제입니다. 주로 버그 재생산의 이전 작업 충돌 처리 [5], [6]; 많은 버그 보고서가 의미론적 문제를 다루기 때문에 범위가 실제로 제한됩니다.
일반적인 보고서-테스트 문제는 소프트웨어 엔지니어링 커뮤니티에 매우 중요합니다. 이 문제를 해결하면 개발자가 보고된 버그를 재현하는 테스트 사례를 갖춘 자동화된 디버깅 기술을 더 많이 사용할 수 있기 때문입니다. Koyuncuet al. [7] 널리 사용되는 Defects4J [8] 버그 벤치마크에서 96%의 사례에서 버그 보고서가 제출되기 전에는 버그 공개 테스트가 존재하지 않았습니다. 결과적으로 버그 재현 테스트[9,10]에 의존하기 때문에 버그가 처음 보고될 때 종종 Defects4J에서 평가되는 최신 자동 디버깅 기술을 활용하기 어려울 수 있습니다. 반대로 버그를 드러내는 테스트를 자동으로 생성하는 기술과 함께 광범위한 자동 디버깅 기술을 사용할 수 있게 됩니다.
이 문제를 해결하기 위한 초기 시도로 대규모 언어 모델(LLM)이 테스트를 생성하도록 제안합니다. 우리가 LLM을 사용하는 이유는 광범위한 자연어 처리 작업[11] 및 프로그래밍 작업[12]에 대한 인상적인 성능을 기반으로 합니다. **이 작업에서는 버그 보고서에서 테스트 사례를 생성하도록 기능을 확장할 수 있는지 여부를 살펴봅니다. 더 중요한 것은 이 문제에 적용될 때 LLM의 성능이 언제 LLM이 생성하는 테스트에 의존할 수 있는지에 대한 문제와 함께 연구되어야 한다고 주장합니다. 이러한 질문은 실제 개발자 사용에 중요합니다. Sarkar et al. [13] 관련 예제를 제공하여 개발자가 LLM이 코드 생성에 사용될 때 입찰을 수행하는 시기를 이해하는 데 어려움을 겪고 있음을 보여줍니다. 이러한 지식 격차를 메우기 위해 우리는 OpenAI LLM, Codex [14]가 테스트를 생성하고 결과를 처리하며 버그가 있다고 합리적으로 확신할 수 있는 경우에만 솔루션을 제안하는 프레임워크인 LIBRO(LLM Induced Bug ReprOduction)를 제안합니다. 재현에 성공했습니다.**
우리는 LIBRO의 성공적인 버그 재생산을 나타낼 수 있는 기능을 식별하기 위해 Defects4J 벤치마크와 우리가 구성한 새로운 보고서-테스트 데이터 세트에 대해 광범위한 경험적 실험을 수행합니다. 우리는 Defects4J 벤치마크의 경우 LIBRO가 251개의 버그 또는 버그 보고서에서 연구된 모든 버그의 33.5%에 대해 적어도 하나의 버그 재현 테스트를 생성할 수 있음을 발견했습니다. LIBRO는 또한 어떤 버그 재현 시도가 성공했는지 71.4%의 정확도로 성공적으로 추론하고 149개의 버그에 대한 첫 번째 제안으로 실제 버그 재현 테스트를 생성했습니다. 추가 검증을 위해 우리는 우리가 구축한 최근 버그 보고서 데이터 세트에서 LIBRO를 평가하여 이 개별 데이터 세트에서도 버그의 32.2%를 재현할 수 있음을 발견하고 이 다른 데이터 세트에서도 휴리스틱을 제안하는 테스트를 확인했습니다.
요약하면, 우리의 기여는 다음과 같습니다.
• 버그 재현 테스트 생성의 중요성을 확인하기 위해 오픈 소스 리포지토리 분석을 수행합니다.
버그 보고서의 사례;
• 재생산을 위해 LLM을 활용하는 프레임워크를 제안합니다.
버그, 생성된 테스트를 개발자에게만 제안
결과가 신뢰할 수 있을 때;
• 두 가지 데이터 세트에 대해 광범위한 경험적 분석을 수행합니다.
우리가 찾은 패턴과 그에 따른 성과는
LIBRO의 manance는 견고합니다.
논문의 나머지 부분은 다음과 같이 구성된다. 우리 섹션 II에서 우리의 연구에 동기를 부여하십시오. 이를 바탕으로 섹션 III에서 접근 방식을 설명합니다. 평가 설정 s 및 연구 질문은 각각 섹션 IV 및 섹션 V에 있습니다. 결과는 섹션 VI에 제시되며 유효성에 대한 위협은 섹션 VII에서 논의됩니다. 섹션 VIII는 관련 문헌의 개요를 제공하고 섹션 IX는 결론을 내립니다.

---

## 동기

이전 섹션에서 설명한 것처럼 보고-테스트 문제의 중요성은 두 가지 관찰에 있습니다. 첫 번째는 자동화된 디버깅 기술이 종종 [9], [10]라고 가정하는 것과는 달리 버그 보고서가 제출될 때 버그 공개 테스트를 거의 사용할 수 없다는 것입니다. Koyuncuet al. [7] **SBFL(Spectrum-Based Fault Localization) 기법이 분석한 사례의 95%에서 보고된 시점의 버그를 찾을 수 없다고 보고하여 완전 정적 자동 디버깅 기법을 제안한다. 그러나 Le et al. [15] 동적 정보를 사용하면 더 정확한 현지화 결과를 얻을 수 있음을 보여줍니다. 이와 같이 보고서-테스트 기술은 자동화된 디버깅 문헌의 상당 부분의 실용성 및/또는 성능을 향상시킬 수 있습니다.**
**또 다른 관찰은 보고서 대 테스트 문제가 아마도 과소 평가되었지만 그럼에도 불구하고 중요하다는 것입니다.**
테스트의 반복 부분. 개발자에 대한 기존 설문 조사에 따르면 **개발자는 자동화된 테스트를 개선하는 방법으로 버그 보고서에서 테스트를 생성하는 것을 고려하고 있습니다.** Daka와 Fraser[16]는 225명의 **소프트웨어 개발자를 대상으로 설문조사를 실시하고 자동화된 테스트 생성이 개발자에게 도움이 될 수 있는 방법을 지적합니다. 그 중 3가지(테스트 대상 결정, 사실성, 확인 대상 결정)는 버그 보고서를 사용하여 해결할 수 있습니다.** 비교적 잘 정의된 활동. Kochharet al. [17] 수백 명의 개발자에게 "유지보수 중에 버그가 수정되면 이를 다루는 테스트 케이스를 추가하는 것이 좋습니다"라는 진술에 동의하는지 명시적으로 물어보고 리커트 척도에서 평균 4.4라는 강력한 동의를 얻습니다. 5.
개발자가 정기적으로 보고서-테스트 문제를 처리하는지 추가로 확인하기 위해 수백 개의 오픈 소스 Java 리포지토리를 마이닝하여 버그 보고서에 기인할 수 있는 테스트 추가 수를 분석합니다. Alon 등의 Java-med 데이터 세트로 시작합니다. [18], GitHub의 최고 인기 Java 프로젝트 1000개로 구성되어 있습니다. 각 저장소의 커밋 목록에서 (i) 커밋이 테스트를 추가하는지, (ii) 커밋이 이슈와 연결되어 있는지 확인합니다. 커밋이 테스트를 추가하는지 확인하기 위해 diff가 테스트 본문과 함께 @Test 데코레이터를 추가하는지 확인합니다. 또한 (i) 커밋 메시지에 "(fixes/resolves/closes) #NUM"이 언급되거나 (ii) 커밋 메시지에 풀 요청이 언급된 경우 커밋을 버그 보고서(또는 GitHub의 문제)에 연결합니다. 차례로 문제를 언급합니다. 이러한 보고서 관련 커밋에 의해 추가된 테스트 수를 현재(2022년 8월) 테스트 스위트의 크기와 비교하여 이러한 테스트의 보급률을 추정합니다. 리포지토리마다 문제 처리 방식이 다르기 때문에 테스트를 추가하는 문제 관련 커밋이 없는 리포지토리를 필터링합니다. 이는 다른 버그 처리 방식(예: Google guava)을 나타내기 때문입니다. 따라서 표 1과 같이 300개의 저장소를 분석한다.

![스크린샷 2023-02-15 오후 11.41.15](/Users/choejunhyeog/Desktop/스크린샷 2023-02-15 오후 11.41.15.png)

현재 테스트 스위트 크기와 관련하여 해당 300개의 리포지토리에서 문제 참조 커밋에 의해 추가된 테스트의 중앙값 비율은 28.4%로 버그 보고로 인해 상당한 수의 테스트가 추가되었음을 나타냅니다. 테스트가 추가된 후 테스트에 어떤 일이 발생하는지 추적하지 않기 때문에 이것이 테스트 스위트의 테스트 중 28.4%가 버그 보고서에서 비롯되었다는 의미는 아닙니다. 그럼에도 불구하고 보고서에서 테스트 활동이 테스트 스위트의 발전에 중요한 역할을 한다는 것을 나타냅니다. 이 결과를 바탕으로 우리는 report-to-test 생성 문제가 오픈 소스 개발자에 의해 정기적으로 처리된다는 결론을 내립니다. 확장하여 확인된 테스트를 제안 및/또는 자동으로 커밋하는 자동화된 보고서-테스트 기술은 개발자의 자연스러운 워크플로에 도움이 될 것입니다.
문제의 중요성에도 불구하고 일반적인 형태는 해결하기 어려운 문제로 남아 있습니다. 기존 작업은 다른 문제에 집중하여 문제의 특수한 경우를 해결하려고 시도합니다.
측면: 일부는 보고서의 문장을 관찰된 또는 예상되는 동작과 같은 범주로 분류하는 반면, 다른 일부는 충돌만 재현합니다(충돌 재현) [6], [20]. 우리는 이 문제를 해결하려면 추론을 수행하는 기능은 말할 것도 없고 자연 언어와 프로그래밍 언어에 대한 충분한 이해가 필요하다는 것을 관찰했습니다. 예를 들어, 표 II의 버그 보고서는 코드를 명시적으로 지정하지 않지만 영어와 Java에 능통한 사용자는 두 인수가 모두 NaN인 경우 'MathUtils'의 'equals' 메서드가 false를 반환해야 한다고 추론할 수 있습니다.
유망한 솔루션 중 하나는 사전 훈련된 LLM(대형 언어 모델)의 기능을 활용하는 것입니다. LLM은 일반적으로 언어 모델링 목표, 즉 이전 컨텍스트를 기반으로 다음 토큰을 예측하도록 훈련된 트랜스포머 기반 신경망[13]입니다. 그들의 주요 참신함 중 하나는 교육 없이 작업을 수행할 수 있다는 것입니다. 텍스트 프롬프트를 통해 작업을 수행하도록 LLM에 '요청'하기만 하면 LLM이 실제로 작업을 수행할 수 있는 경우가 많습니다[11]. 따라서 한 가지 궁금한 점은 LLM이 주어진 보고서에서 얼마나 많은 버그를 재현할 수 있는지입니다. 반면에 실질적으로 중요한 것은 서론에서 언급한 바와 같이 LLM 결과를 믿고 사용해야 할 때를 알 수 있다는 것입니다. 이를 위해 우리는 높은 정밀도를 나타내는 휴리스틱을 찾는 데 집중하고 개발자가 LIBRO를 사용할 때 처리해야 하는 번거로움을 최소화합니다.

---

## 접근법

![스크린샷 2023-02-15 오후 11.42.06](/Users/choejunhyeog/Desktop/스크린샷 2023-02-15 오후 11.42.06.png)

우리 접근 방식의 개요 다이어그램이 그림 1에 나와 있습니다. 버그 보고서가 주어지면 LIBRO는 먼저 LLM을 쿼리하라는 프롬프트를 구성합니다(그림 1:(A)). 이 프롬프트를 사용하여 LLM을 여러 번 쿼리하여 테스트 후보의 초기 집합을 생성합니다(그림 1:(B)). 그런 다음 LIBRO는 대상 프로그램에서 실행 가능하도록 테스트를 처리합니다(그림 1:(C)). 이후에 LIBRO는 버그를 재현할 가능성이 있는 테스트를 식별하고 조정하며, 그렇다면 개발자 검사 노력을 최소화하기 위해 순위를 매깁니다(그림 1:(D)). 이 섹션의 나머지 부분에서는 표 II에 제공된 실행 예제를 사용하여 각 단계를 자세히 설명합니다.

### A. Prompt Engineering

LLM은 핵심적으로 대규모 자동완성 신경망입니다. 이전 작업에서는 **LLM에 문제 해결을 '요청'하는 다양한 방법이 성능 수준을 크게 변화시킬 것이라는 사실을 발견했습니다[21]. 주어진 작업을 수행하기 위한 최상의 쿼리를 찾는 것을 프롬프트 엔지니어링이라고 합니다[22].**
지정된 버그 보고서에서 테스트 메서드를 생성하는 LLM을 만들기 위해 **버그 보고서에서 프롬프트에 사용할 Markdown 문서를 구성**합니다. Listing 1의 예를 고려하십시오. 버그 보고서는 표 II에 나와 있습니다. LIBRO는 Markdown 문서에 몇 가지 고유한 부분을 추가합니다. 그의 역할은 LLM이 테스트 방법을 작성하도록 유도하는 것입니다.

![스크린샷 2023-02-16 오전 12.03.56](/Users/choejunhyeog/Desktop/스크린샷 2023-02-16 오전 12.03.56.png)

이 기본 프롬프트의 다양한 변형을 평가합니다.
Brownet al. [11] LLM은 프롬프트에 제공된 질문-답변 예제의 이점을 보고합니다. 우리의 경우 이는 버그 보고서(질문) 및 해당 버그 재현 테스트(답변)의 예를 제공하는 것을 의미합니다. 이를 염두에 두고 다양한 예제를 실험하여 더 많은 예제를 추가하는지, 동일한 프로젝트 내에서 또는 다른 프로젝트에서 예제를 갖는 것이 성능에 상당한 영향을 미치는지 확인합니다.
프롬프트 형식에 실제 제한이 없기 때문에 충돌 버그에 대한 스택 추적을 제공하거나(스택 추적이 제공된 상황을 시뮬레이트하기 위해) 오류가 있는 클래스의 생성자를 제공하여(예: 버그의 위치가 보고됨).
우리의 특정 템플릿 형식은 우리가 생성하는 프롬프트가 LLM 교육 데이터 내에 그대로 존재할 가능성이 거의 없습니다. 또한 실제로 대부분의 보고서는 참조 체인을 통해서만 버그 공개 테스트에 연결됩니다. 따라서 우리의 형식은 원고의 뒷부분에 설명된 이 위협을 제한하기 위해 취한 다른 조치 중에서 데이터 유출 문제를 부분적으로 완화합니다.



### B. Quering an LLM

LIBRO는 생성된 프롬프트를 사용하여 LLM에 쿼리하여 프롬프트를 따르는 토큰을 예측합니다. 프롬프트의 특성으로 인해 특히 프롬프트가 시퀀스 public void test로 끝나기 때문에 테스트 메서드를 생성할 가능성이 높습니다. Markdown에서 코드 블록의 끝을 나타내는 문자열이 처음 나타날 때까지 토큰을 수락하여 결과가 테스트 메서드에만 적용되도록 합니다.
완전히 탐욕적인 디코딩(즉, 가장 가능성이 높은 다음 토큰을 기반으로 엄격하게 디코딩)을 수행할 때 LLM이 열등한 결과를 산출하는 것으로 알려져 있습니다.
온도 매개변수에 의해 조절되는 동작인 가중 무작위 샘플링을 수행할 때 성능이 더 좋습니다. 이전 작업에 이어 온도를 0.7 [11]로 설정하여 LLM이 정확히 동일한 프롬프트를 기반으로 여러 개의 개별 테스트를 생성할 수 있도록 합니다. 우리는 여러 후보 재현 테스트를 생성한 다음 해당 특성을 사용하여 버그가 실제로 재현될 가능성을 식별하는 접근 방식을 취합니다.

Listing 1의 프롬프트가 지정된 LLM의 출력 예는 Listing 2에 나와 있습니다. 이 시점에서 LLM의 출력은 일반적으로 자체적으로 컴파일할 수 없으며 import 문과 같은 다른 구성이 필요합니다. 다음으로 LIBRO가 생성된 테스트를 기존 테스트 스위트에 통합하여 실행 가능하게 만드는 방법을 제시합니다.

![스크린샷 2023-02-26 오후 10.05.28](/Users/choejunhyeog/Desktop/스크린샷 2023-02-26 오후 10.05.28.png)



### C. Test Postprocessing

먼저 LIBRO가 테스트 방법을 기존 제품군에 주입하는 방법을 설명하고 LIBRO가 충족되지 않은 나머지 종속성을 해결하는 방법을 설명합니다.
1) 적절한 테스트 클래스에 테스트 주입: 개발자가 버그 보고서에서 테스트 메서드를 찾으면 테스트 메서드에 필요한 컨텍스트(예: 필수 종속성)를 제공하는 테스트 클래스에 삽입할 가능성이 높습니다. 예를 들어 실행 중인 예제의 버그에 대해 개발자는 MathUtilsTest 클래스에 재현 테스트를 추가했습니다. 여기서 초점 클래스인 MathUtils를 포함하여 대부분의 필수 종속성을 이미 가져왔습니다. 따라서 초기에 충족되지 않은 상당한 수의 종속성을 해결하면서 개발자 워크플로우와 일치하므로 LLM 생성 테스트를 기존 테스트 클래스에 주입하는 것도 당연합니다.

![스크린샷 2023-02-26 오후 10.14.06](/Users/choejunhyeog/Desktop/스크린샷 2023-02-26 오후 10.14.06.png)



![스크린샷 2023-02-26 오후 10.15.22](/Users/choejunhyeog/Desktop/스크린샷 2023-02-26 오후 10.15.22.png)

테스트 메서드를 주입할 최상의 테스트 클래스를 찾기 위해 생성된 테스트와 어휘적으로 가장 유사한 테스트 클래스를 찾습니다(알고리즘 1, 1행). 테스트 메서드가 테스트 클래스에 속하는 경우 테스트 메서드는 비슷한 메서드와 클래스를 사용할 가능성이 높으므로 해당 테스트 클래스의 다른 테스트와 어휘적으로 관련되어 있습니다. 공식적으로 방정식 (1)을 기반으로 각 테스트 클래스에 대해 일치하는 점수를 할당합니다.

![스크린샷 2023-02-26 오후 10.15.33](/Users/choejunhyeog/Desktop/스크린샷 2023-02-26 오후 10.15.33.png)

여기서 Tt와 Tci는 각각 생성된 테스트 메서드와 i번째 테스트 클래스의 토큰 집합입니다. 예를 들어 Listing 3은 MathUtilsTest 클래스의 핵심 명령문을 보여줍니다. 여기서 테스트 클래스에는 Listing 2, 특히 4행과 6행의 LLM 생성 테스트에서 사용하는 것과 유사한 메서드 호출 및 상수가 포함되어 있습니다.
온전한 검사로서 Defects4J 벤치마크의 Math 및 Lang 프로젝트에서 실제 개발자가 추가한 버그 재현 테스트를 주입하고 알고리즘 1을 기반으로 정상적으로 실행되는지 확인합니다. 89%의 시간 동안 실행이 정상적으로 진행되는 것을 확인했습니다. , 알고리즘이 테스트를 실행할 수 있는 환경을 합리적으로 찾는다고 제안합니다.

2) 나머지 종속성 해결: 테스트를 올바른 클래스에 배치하여 많은 종속성 문제가 해결되지만 테스트에서 가져와야 하는 새로운 구성이 도입될 수 있습니다. 이러한 경우를 처리하기 위해 LIBRO는 가져올 패키지를 경험적으로 유추합니다.
알고리즘 1의 2~10행은 LIBRO의 종속성 해결 프로세스를 설명합니다. 먼저 LIBRO는 생성된 테스트 메서드를 구문 분석하고 변수 유형과 참조된 클래스 이름/생성자/예외를 식별합니다. 그런 다음 LIBRO는 이름을 테스트 클래스의 기존 가져오기 문과 어휘적으로 일치시켜 "이미 가져온" 클래스 이름을 필터링합니다(3행).
이 프로세스의 결과로 테스트 클래스 내에서 해결되지 않은 유형을 찾습니다. LIBRO는 먼저 유형의 식별된 이름을 가진 공용 클래스를 찾으려고 시도합니다. 그러한 파일이 정확히 하나 있으면 식별된 클래스에 대한 클래스 경로가 파생되고(7행) import 문이 추가됩니다(11행). 그러나 일치하는 클래스가 없거나 여러 개가 존재할 수 있습니다. 두 경우 모두 LIBRO는 프로젝트 내에서 대상 클래스 이름으로 끝나는 import 문을 찾습니다(예: MathUtils를 검색할 때 LIBRO는 import .*MathUtils;를 찾습니다). LIBRO는 모든 프로젝트 소스 코드 파일에서 가장 일반적인 import 문을 선택합니다. 또한 프로젝트 자체 내에 적절한 가져오기가 없는 경우에도 어설션 문을 적절하게 가져올 수 있도록 하는 몇 가지 규칙을 추가합니다.
우리의 후처리 파이프라인은 모든 경우에 컴파일을 보장하지는 않지만 LIBRO에서 사용하는 휴리스틱은 원시 테스트 방법의 처리되지 않은 종속성 대부분을 해결할 수 있습니다. 사후 처리 단계를 거친 후 LIBRO는 후보 버그 재현 테스트를 식별하기 위해 테스트를 실행합니다.



### D. Selection and Ranking

![스크린샷 2023-02-27 오전 12.22.30](/Users/choejunhyeog/Desktop/스크린샷 2023-02-27 오전 12.22.30.png)

테스트는 보고서에 지정된 버그로 인해 테스트가 실패하는 경우에만 BRT(Bug Reproducing Test)입니다. LIBRO에서 생성한 테스트가 BRT가 되기 위한 필수 조건은 테스트가 버그가 있는 프로그램에서 컴파일되고 실패한다는 것입니다. 이러한 테스트를 FIB(Fail In the Buggy 프로그램) 테스트라고 합니다. 그러나 모든 FIB 테스트가 BRT가 아니므로 버그 재생산이 성공했는지 여부를 말하기가 어렵습니다. 이것은 충돌 재현 작업[20]과 우리를 구분하는 한 가지 요소인데, 충돌 재현 기술은 충돌 당시의 스택 추적을 비교하여 버그가 재현되었는지 여부를 확인할 수 있기 때문입니다. 반면에 개발자에게 여러 솔루션을 반복하도록 요청하는 것은 일반적으로 바람직하지 않기 때문에 생성된 모든 FIB 테스트를 개발자에게 제시하는 것은 현명하지 않습니다[23], [24]. 이와 같이 LIBRO는 테스트를 제안할 시기를 결정하고 제안하는 경우 성공적인 버그 재현과 상관 관계가 있는 여러 패턴을 사용하여 제안할 테스트를 결정하려고 시도합니다.
알고리즘 2는 LIBRO가 결과를 제시할지 여부를 결정하는 방법과 제시할 경우 생성된 테스트의 순위를 매기는 방법을 설명합니다. 1-10행에서 LIBRO는 먼저 개발자에게 결과를 전혀 표시할지 여부를 결정합니다(선택). 동일한 실패 출력(동일한 오류 유형 및 오류 메시지)을 나타내는 FIB 테스트를 그룹화하고 동일한 그룹의 테스트 수(8행의 max_output_clus_size)를 확인합니다. 이는 여러 테스트에서 유사한 실패 동작이 나타나면 LLM의 독립적인 예측이 서로 '동의'하기 때문에 LLM이 예측에 '자신감'이 있을 가능성이 높고 버그가 재현될 가능성이 높다는 직관에 기반합니다. 성공했습니다. LIBRO는 출력에 상당한 동의가 있는 경우에만 결과를 표시하거나(합의 임계값 T hr을 높게 설정) 더 많은 탐색 결과를 표시하도록(Thr을 낮게 설정) 구성할 수 있습니다.
결과를 보여 주기로 결정하면 LIBRO는 세 가지 휴리스틱에 의존하여 생성된 테스트의 순위를 차별적 강도를 높이는 순서로 지정합니다. 첫째, **실패 메시지 및/또는 테스트 코드가 버그 보고서에서 관찰되고 언급된 동작(예외 또는 출력 값)을 표시하는 경우 테스트에서 버그가 재생산될 수 있습니다.** 이 휴리스틱은 정확하지만 테스트를 '포함됨'과 '포함되지 않음'의 그룹으로만 나눌 수 있기 때문에 그 결정은 그다지 차별적이지 않습니다. **다음으로 LLM 세대의 '합의'를 나타내는 출력 클러스터 크기(output_clus_size)를 살펴보고 생성된 테스트 간의 '합의'를 살펴봅니다.** 마지막으로 **LIBRO는 가장 세밀한 신호인 테스트 길이(짧은 테스트가 이해하기 쉬움)를 기준으로 우선 순위를 지정합니다.** 먼저 구문적으로 고유한 테스트만 남겨두고(11행) 위의 휴리스틱을 사용하여 해당 클러스터 내에서 출력 클러스터와 테스트를 정렬합니다(16행 및 18행).
동일한 실패 출력을 가진 테스트는 서로 유사하므로 클러스터의 한 테스트가 BRT가 아닌 경우 동일한 클러스터의 나머지 테스트도 BRT가 아닐 가능성이 높습니다. 따라서 LIBRO는 다양한 클러스터 배열의 테스트를 보여줍니다. 19-22행의 각 i번째 반복에 대해 각 클러스터의 i번째 순위 테스트가 선택되어 목록에 추가됩니다.



---

## 평가



---

## 결과

A. RQ1. LIBRO는 얼마나 효과적입니까?
1) RQ1-1: 표 III은 어떤 프롬프트/정보 설정이 가장 잘 작동하는지 보여줍니다. 소스 프로젝트의 예제를 사용할 때 해당 프로젝트 내에서 사용 가능한 가장 오래된 테스트를 사용합니다. 그렇지 않으면 모든 프로젝트에서 두 개의 직접 선택한 보고서-테스트 쌍(Time-24, Lang-1)을 사용합니다. 우리는 생성자(à la AthenaTest [28])를 제공하는 것이 크게 도움이 되지 않는다는 것을 발견했지만 스택 추적을 추가하면 크래시 버그를 재현하는 데 도움이 되므로 LIBRO가 문제를 더 정확하게 복제하기 위해 스택 정보를 사용함으로써 이점을 얻을 수 있음을 나타냅니다. 흥미롭게도 프로젝트 내 예제를 추가하면 성능이 저하됩니다. 이러한 사례를 조사한 결과 LIBRO가 제공되지 않아야 할 경우에도 단순히 제공된 예제를 복사하여 성능이 저하된 것으로 나타났습니다. 또한 예제의 수가 상당한 차이를 만들어 내는 것을 발견하여(기본 설정의 n=50 결과에서 두 개의 예제 n=10 값이 샘플링됨) 예제를 추가하면 성능이 향상되는 데 도움이 된다는 기존 결과를 확인합니다. 결과적으로, RQ2-1에서 더 자세히 살펴보듯이 예제의 수는 LLM이 쿼리되는 횟수보다 덜 중요한 것 같습니다. 두 가지 예시 50회 반복 설정이 최고의 성능을 보여주므로 나머지 논문에서는 기본 설정으로 사용합니다.

![스크린샷 2023-03-05 오후 9.49.32](/Users/choejunhyeog/Desktop/스크린샷 2023-03-05 오후 9.49.32.png)

두 가지 예시 50회 반복 설정에서 우리는 전체 251개 버그, 즉 연구된 Defects4J 버그 750개 중 33.5%가 LIBRO에 의해 재현됨을 발견했습니다. 표 IV는 프로젝트별 성능 분석을 나타냅니다. 모든 프로젝트에 적어도 하나의 버그가 재현되지만 재현되는 버그의 비율은 크게 다를 수 있습니다. 예를 들어, LIBRO는 고유한 테스트 구조를 갖는 것으로 알려진 Closure 프로젝트에서 소수의 버그를 재현합니다[29]. 반면에 Lang 또는 Jsoup 프로젝트의 성능은 일반적으로 독립적이고 단순하며 테스트가 더 강력합니다.

![스크린샷 2023-03-05 오후 9.50.11](/Users/choejunhyeog/Desktop/스크린샷 2023-03-05 오후 9.50.11.png)

RQ1-1에 대한 답변: 많은(251) 버그가 자동으로 복제될 수 있으며 버그는 다양한 프로젝트 그룹에 걸쳐 복제됩니다. 또한 프롬프트의 예제 수와 생성 시도 횟수는 성능에 큰 영향을 미칩니다.

---

*2) RQ1-2:* 우리는 LIBRO를 최신 충돌 재현 기술인 EvoCrash 및 버그 보고서의 코드 스니펫을 사용하는 'Copy&Paste 기준선'과 비교합니다. 비교 결과는 그림 2에 나와 있습니다. 우리는 LIBRO가 다른 기준에 비해 크고 뚜렷한 버그 그룹을 복제한다는 것을 발견했습니다. LIBRO는 EvoCrash보다 91개의 고유한 버그(19개는 충돌 버그임)를 재현했으며, 이는 LIBRO가 이전 작업에서 처리할 수 없었던 비충돌 버그를 재현할 수 있음을 보여줍니다(그림 2(b)). 반면 복사 및 붙여넣기 기준선은 BRT가 때때로 버그 보고서에 포함되지만 보고-테스트 작업이 전혀 사소하지 않다는 것을 보여줍니다. 흥미롭게도 복사 및 붙여넣기 기준선에 의해 재현된 8개의 버그는 LIBRO에 의해 재현되지 않았습니다. 우리는 이것이 LIBRO의 생성 길이를 초과하는 긴 테스트 때문이거나 복잡한 헬퍼 함수에 대한 의존성 때문이라는 것을 발견했습니다.

![스크린샷 2023-03-05 오후 9.51.57](/Users/choejunhyeog/Desktop/스크린샷 2023-03-05 오후 9.51.57.png)

RQ1-2에 대한 답변: LIBRO는 이전 작업과 관련하여 크고 뚜렷한 버그 그룹을 복제할 수 있습니다.

---

B. RQ2. LIBRO는 얼마나 효율적입니까?
1) RQ2-1: 여기서는 특정 버그 재현 성능을 얻기 위해 얼마나 많은 테스트를 생성해야 하는지 조사합니다. 이를 위해 각 Defects4J 버그에 대해 기본 설정에서 생성된 50개 테스트 중 x개의 테스트를 무작위로 샘플링하여 버그당 테스트 수를 줄였습니다. 그런 다음 샘플링된 테스트만 사용할 때 재현되는 버그 수를 확인합니다. 분포를 근사하기 위해 이 과정을 1,000번 반복합니다.
결과는 그림 3에 나와 있습니다. x축은 로그 스케일입니다. 흥미롭게도 우리는 테스트 생성 시도 횟수와 평균 버그 재현 성능 사이에 로그 관계가 있음을 발견했습니다. 이것은 단순히 더 많은 테스트를 생성하여 더 많은 버그를 복제하는 것이 점점 더 어려워지고 있음을 시사합니다. 그래프에 안정기의 징후가 보이지 않으므로 훨씬 더 많은 테스트 샘플로 실험하면 더 나은 버그 재현 결과를 얻을 수 있습니다.

RQ2-1에 대한 답변: 재현된 버그의 수는 성능 정체의 징후 없이 생성된 테스트 수에 대해 대수적으로 증가합니다.

---

2) RQ2-2: 파이프라인의 각 단계를 수행하는 데 걸리는 시간을 표 V에 보고합니다. API 쿼리에 가장 많은 시간이 소요되며 약 5.85초가 소요됩니다. 사후 처리 및 테스트 실행은 테스트당(테스트 실행 시) 각각 1.23초 및 4초가 걸립니다. 전반적으로 LIBRO는 50개의 테스트를 생성하고 처리하는 데 평균 444초가 걸렸으며, 이는 검색 기반 기술에서 자주 사용하는 10분 검색 예산 범위 내에 있습니다[20].

![스크린샷 2023-03-05 오후 10.46.52](/Users/choejunhyeog/Desktop/스크린샷 2023-03-05 오후 10.46.52.png)

RQ2-2에 대한 답변: 우리의 시간 측정에 따르면 LIBRO는 사용하는 데 다른 방법보다 훨씬 더 긴 시간이 걸리지 않습니다.

---

3) RQ2-3: 이 연구 질문을 통해 LIBRO가 선택 및 순위 지정 절차를 통해 버그 재현 테스트의 우선 순위를 얼마나 효과적으로 지정하는지 측정합니다. LIBRO는 섹션 III-D의 특정 합의 임계값 이상의 결과만 표시하므로 먼저 재생산된 총 버그 수와 정밀도(즉, LIBRO에서 선택한 모든 버그 중에서 성공적으로 재생산된 버그의 비율) 사이의 균형을 제시합니다. 그림 4에서. 임계값을 높이면 더 많은 제안(BRT 포함)이 삭제되지만 정밀도는 높아져 선택 임계값을 조정하여 정밀도를 원활하게 높일 수 있음을 나타냅니다.
우리는 구체적으로 합의 임계값을 1로 설정했습니다.
재현된 버그를 최대한 많이 보존하기 위한 가치
가능한 한. FIB가 있는 570개의 버그 중 350개의 버그
선택됩니다. 그 350개 중 219개가 재생산됩니다(
정밀도는 0.63(= 219)인 반면 재현율(즉, 비율 350)
전체 재현된 버그 중 선택된 재현된 버그의 수)는
0.87(= 219). 반대로 선택 과정에서 재현되지 않은 188개의 버그를 필터링하고 재현에 성공한 몇 개의 버그만 제외합니다. 임계값을 보다 공격적인 값인 10으로 설정하면 0.42의 재현율에 대해 0.84의 더 높은 정밀도를 얻을 수 있습니다. 어쨌든 그림 4에서 볼 수 있듯이 우리의 선택 기술은 무작위보다 훨씬 우수하여 개발자 리소스를 절약할 수 있음을 나타냅니다.
선택된 버그 중에서 LIBRO의 테스트 순위가 무작위 기준선에 대해 얼마나 효과적인지 평가합니다. 무작위 접근법은 생성된 테스트의 구문 클러스터(구문적으로 동등한 FIB 테스트 그룹)의 순위를 임의로 지정합니다. 무작위 기준선을 100번 실행하고 결과의 평균을 냅니다.
표 VI는 순위 평가 결과를 나타냅니다. Defects4J 벤치마크에서 LIBRO의 순위 지정 기법은 모든 acc@n 메트릭에 걸쳐 무작위 기준선에서 향상되어 n = 1, 3 및 5에서 각각 무작위 기준선보다 30, 14 및 7 더 많은 BRT를 나타냅니다. acc@1과 관련하여 첫 번째 열은 LIBRO에서 생성한 상위 순위 테스트의 43%가 첫 번째 시도에서 원래 버그 보고서를 성공적으로 재현했음을 보여줍니다. n이 5로 증가하면 선택한 버그의 57% 또는 재현된 모든 버그의 80%에서 BRT를 찾을 수 있습니다. 여기에서 보수적인 임계값 선택은 정밀도보다 재현율을 강조합니다. 그러나 임계값을 높이면 최대 정밀도가 0.8까지 올라갈 수 있습니다(T hr = 10, n = 5의 경우).
wef@nagg 값은 선택된 모든 버그(350)의 wef@n을 합산하고 평균화하여 추가로 보고됩니다. 합산된 wef@n 값은 상위 n 순위 테스트 내에서 수동으로 검사되는 비 BRT의 총 수를 나타냅니다. wef@n 값이 작을수록 기술이 더 많은 버그 재현 테스트를 제공한다는 것을 나타냅니다. 전반적으로 LIBRO의 순위는 버그를 선택한 후에도 무작위 기준선과 비교할 때 낭비되는 노력을 최대 14.5%까지 절약합니다. 이러한 결과를 바탕으로 우리는 LIBRO가 낭비되는 검사 노력을 줄이고 따라서 개발자를 지원하는 데 유용할 수 있다는 결론을 내립니다.

RQ2-3에 대한 답변: LIBRO는 검사해야 하는 버그와 테스트의 수를 모두 줄일 수 있습니다. 성공적인 버그 재생산의 87%를 보존하면서 버그의 33%를 안전하게 폐기합니다. 선택된 버그 세트 중에서 모든 버그 복제의 80%는 5번의 검사 내에서 찾을 수 있습니다.

---

C.RQ3. LIBRO는 실제로 얼마나 잘 작동할까요?

1) RQ3-1: 최근 버그 보고서의 GHRB 데이터 세트에서 작동할 때 LIBRO의 성능을 탐색합니다. 우리가 연구한 31개의 버그 보고서 중 LIBRO는 50번의 시도를 기반으로 10개의 버그에 대한 버그 재현 테스트를 자동으로 생성할 수 있으며 성공률은 32.2%입니다. 이 성공률은 RQ1-1에 제시된 Defects4J의 결과와 유사하며 LIBRO가 새로운 버그 보고서로 일반화되었음을 시사합니다. 프로젝트별 결과 분석은 표 VII에 나와 있습니다. 버그는 AssertJ, Jsoup, Gson 및 sslcontext에서 성공적으로 재현되었지만 다른 두 개에서는 재현되지 않았습니다. 많은 수의 버그가 있음에도 불구하고 Checkstyle 프로젝트에서 버그를 재현할 수 없었습니다. 조사 결과 섹션 VI-C3에서 볼 수 있듯이 프로젝트의 테스트가 LIBRO가 액세스할 수 없는 외부 파일에 크게 의존하기 때문이라는 것을 알게 되었습니다. LIBRO는 또한 Jackson 프로젝트에 대한 BRT를 생성하지 않지만 Jackson 프로젝트의 버그 수가 적기 때문에 결론을 내리기가 어렵습니다.

RQ3-1에 대한 답변: LIBRO는 최근 데이터에 대해서도 버그 재현 테스트를 생성할 수 있으며, 이는 단순히 학습한 내용을 기억하는 것이 아님을 시사합니다.

---

2) RQ3-2: LIBRO는 버그 선택 및 테스트 순위 지정을 위해 성공적인 버그 재생산과 관련된 몇 가지 예측 요소를 사용합니다. 이 연구 질문에서는 Defects4J 데이터 세트를 기반으로 식별된 패턴이 최근 GHRB 데이터 세트에서 계속 유지되는지 여부를 확인합니다. 

우리는 최대 출력 클러스터 크기를 FIB 간의 합의 척도로 사용하므로 버그가 재현되었는지 여부를 식별하기 위한 선택 기준으로 사용한다는 점을 상기하십시오. 기준이 예측할 수 있는 신뢰할 수 있는 지표인지 여부를 관찰하기 위해
버그 재현의 성공 여부에 따라 BRT 유무에 관계없이 두 데이터 세트 간의 max_output_clus_size 추세를 관찰합니다. 그림 5에서 BRT가 없는 버그는 일반적으로 max_output_clus_size가 작고 대부분 10 미만임을 알 수 있습니다. 이 패턴은 두 데이터 세트에서 일관됩니다.
GHRB의 순위 결과도 표 VI에 나와 있습니다. 그것들은 Defects4J의 결과와 일치하며 순위 전략에 사용된 기능이 계속해서 성공적인 버그 재생산의 좋은 지표임을 나타냅니다.

RQ3-2에 대한 답변: 우리는 LIBRO의 순위 및 선택에 사용된 요소가 실제 데이터에서 버그 재생산을 일관되게 예측한다는 것을 발견했습니다.

---

3) RQ3-3: LIBRO가 성공하거나 실패한 버그를 재현하려는 시도에 대한 사례 연구를 제시합니다.









---

## 타당성 위협



---

## 관련 연구

A. Test Generation

자동화된 테스트 생성은 거의 50년 전부터 연구되어 왔습니다[2]. 객체 지향 프로그래밍 패러다임의 출현은 테스트 입력 생성 기술의 변화를 가져왔고, 기본 값 탐색에서 적용 범위를 최대화하기 위한 메서드 시퀀스 도출로 이동했습니다[3], [4]. 그러나 이러한 기술의 중요한 문제는 오라클 문제[32]입니다. 테스트에 대한 올바른 동작이 무엇인지 결정하기 어렵기 때문에 자동화된 테스트 생성 기술은 암시적 오라클에 의존하거나[3] 현재 동작을 수락합니다. 올바른 것으로 - 회귀 테스트 [4], [33]를 효과적으로 생성합니다.
우리 작업과 마찬가지로 일부 기술은 사용자가 보고한 문제를 재현하는 데 중점을 둡니다. 일반적으로 사용되는 암시적 오라클은 프로그램이 충돌하지 않아야 한다는 것입니다[32]. 대부분의 기존 작업 [5], [6], [34]–[36]은 사용자가 제공한 것으로 추정되는 스택 추적이 주어진 충돌을 재현하는 것을 목표로 합니다. 반면에 Yakusu[37]와 ReCDroid[38]는 모바일 애플리케이션에 대한 충돌 재현 테스트를 생성하기 위해 특정 형식으로 작성된 사용자 보고서를 요구합니다. 모든 충돌 재현 기술은 충돌 기반 암시적 오라클에 의존하고 SUT 코드를 광범위하게 사용하므로(즉, 화이트박스 기술임) 우리 작업과 크게 다릅니다. BEE[19]는 버그 보고서를 자동으로 구문 분석하여 관찰되거나 예상되는 동작을 설명하지만 실제로 테스트를 생성하는 데는 미치지 못하는 문장을 분류합니다. 우리가 아는 한, Java 프로젝트에서 일반 버그 보고서를 재현하는 기술을 처음으로 제안했습니다.



B. Code Synthesis

코드 합성도 오랜 연구 역사를 가지고 있습니다. 전통적으로 코드 합성은 SyGuS(Syntax-Guided Synthesis)의 맥락에서 SMT 솔버를 통해 접근했습니다[39]. 기계 학습 기술이 향상됨에 따라 코드 합성 작업에서 좋은 성능을 보였습니다. Codex는 LLM이 자연어 설명을 기반으로 프로그래밍 작업을 해결할 수 있음을 보여주었습니다[14]. Codex에 이어 일부 사람들은 코드와 함께 테스트를 합성하는 것이 유용하다는 것을 발견했습니다. AlphaCode는 자동으로 생성된 테스트를 사용하여 코드 합성 성능을 향상시켰고[40], CodeT는 자연 언어 설명에서 테스트와 코드를 공동으로 생성했습니다[12]. 이러한 기술의 초점은 테스트 생성에 있지 않습니다. 반면 LIBRO는 실행 가능성을 극대화하기 위해 LLM 출력을 처리하고 개발자의 인지 부하를 줄이기 위해 테스트 선택/순위 지정에 중점을 둡니다.

---

##  결론

이 백서에서는 먼저 관련 문헌을 조사하고 300개의 오픈 소스 저장소에 대한 분석을 수행하여 보고-테스트 문제가 중요하다는 것을 확립합니다. 이 문제를 해결하기 위해 사전 훈련된 LLM을 사용하여 버그 보고서를 분석하고, 예상 테스트를 생성하고, 마지막으로 여러 간단한 통계를 기반으로 생성된 솔루션의 순위를 매기고 제안하는 기술인 LIBRO를 도입합니다. 광범위한 분석을 통해 우리는 LIBRO가 Defects4J 벤치마크에서 상당한 수의 버그를 재현할 수 있고 기술 사용 요구 사항에 대한 자세한 분석을 수행할 수 있음을 발견했습니다. 수집한 실제 보고서-버그 데이터 세트로 추가 실험을 수행합니다. LIBRO는 Defects4J 벤치마크와 비교할 때 이 데이터 세트에서 유사한 성능을 보여 다재다능함을 보여줍니다. 두 데이터세트에서 LIBRO는 어떤 테스트에서 버그가 재현되는지를 성공적으로 식별하여 LIBRO가 개발자의 노력도 최소화할 수 있음을 보여줍니다. 우리는 이러한 결과를 확장하고 기존 테스트 생성 기술과의 시너지 효과를 탐색하여 실무자에게 도움이 되기를 바랍니다.





